{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle, islice\n",
    "from IPython.display import clear_output\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import operator\n",
    "from functools import reduce\n",
    "import datetime as dt\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import sys\n",
    "import PIL\n",
    "import os\n",
    "__array_ufunc__ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=[]\n",
    "stats_file_list=[]\n",
    "\n",
    "for subdir, dirs, files in os.walk(\"C:\\\\Users\\\\data\"):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if file.endswith(\".csv\"):\n",
    "            if file.endswith(\"stats.csv\"):\n",
    "                stats_file_list.append(filepath)\n",
    "            else:\n",
    "                file_list.append(filepath)\n",
    "\n",
    "print(\"Found\", len(file_list), \"radiance files and\", len(stats_file_list), \"stats files\")\n",
    "assert len(file_list) == len(stats_file_list), \"number of radiance files and stats files don't match\"\n",
    "\n",
    "preview_file = pd.read_csv(file_list[0])\n",
    "preview_file['period'] = pd.to_datetime(preview_file['period']) \n",
    "deltaT = preview_file['period'].max()-preview_file['period'].min()\n",
    "year_count = int(round(deltaT.days/365))\n",
    "print(\"Found\", year_count, \"years of data in first file\")\n",
    "\n",
    "all_columns = preview_file.columns\n",
    "lat_bins = all_columns[2:]\n",
    "lat_bins = lat_bins[::-1]\n",
    "print(\"First file has these\", len(lat_bins), \"latitude bins:\")\n",
    "print(lat_bins)\n",
    "print(\"The other files must have the same bins\")\n",
    "preview_file['wavenumber'] = preview_file['wavenumber'].round(3) # sometimes source data has too many decimal places\n",
    "wavenums = preview_file['wavenumber'].round(3).unique()  # Get all of the unique wavenumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of measurements for trend fitting.\n",
    "# If OLRdata_stats has fewer than MIN_COUNT measurements in it for a given month,\n",
    "# then the trendlines shall ignore that month for the sake of fitting a trendline. \n",
    "MIN_COUNT = 0\n",
    "\n",
    "# Minimum number of months needed to infer a linear trend from each wavenumber+latitude time series\n",
    "# Setting this number lower includes channels that may have failed after weeks or months\n",
    "# Setting this number higher excludes proportionally more thannels that failed or failed and were restored later\n",
    "MIN_MONTHS_TO_OBTAIN_SLOPE = 48\n",
    "\n",
    "# AIRS stability is characterized by others (Strow 2020, Aumann 2016, Pagano) using surface reference temperatures.\n",
    "# Enter the degrees kelvin per year the instrument drifts:\n",
    "drift_K = 0.0023       #Longwave <1700 cm^-1 \n",
    "drift_K2 = 0.008     #Midwave >2100 cm^-1\n",
    "\n",
    "# CO2 band definitions\n",
    "v2start = 649.620-0.1\n",
    "v2end = 681.729+0.1\n",
    "v2wingstart = 687.601-0.1\n",
    "v2wingend = 764.534+0.1\n",
    "v3start = 2195.158\n",
    "v3end = 2395.995\n",
    "win_start = 791.035\n",
    "win_end = 792.814\n",
    "interpolate_start = 681.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress bar function (helpful in next section, it is slow depending on amount of data processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.3f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data and fit linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_masked = total_count = filtered_count = 0\n",
    "i, total_actions = 0, (2*2378*len(lat_bins)+3*len(lat_bins))*len(file_list)\n",
    "all_radiances = {}\n",
    "all_radiance_trends = {}\n",
    "stat_columns = ['cell1', 'cell2', 'cell3', 'cell4']\n",
    "statsdf = pd.DataFrame(index=file_list, columns=stat_columns)\n",
    "statsdf.fillna(0,inplace=True)\n",
    "statsdf2 = pd.DataFrame()\n",
    "\n",
    "for file in file_list:\n",
    "    OLRdata = pd.read_csv(file)\n",
    "    OLRdata['wavenumber'] = OLRdata['wavenumber'].round(3) # sometimes source data has too many decimal places\n",
    "    OLRdata_stats = pd.read_csv(file.rsplit('.', 1)[0]+\"_stats.csv\") # read in stats file for these OLR data\n",
    "    OLRdata_stats.columns = OLRdata.columns # overwrite the columns in the stats file to be identical to OLR data\n",
    "    OLRdata_stats['wavenumber'] = OLRdata_stats['wavenumber'].round(3) # sometimes source data has too many decimal places\n",
    "    \n",
    "    statsdf[\"cell1\"][file]=OLRdata_stats.iloc[:, 2:].count().sum() # Store baseline statistics \"as-read\"\n",
    "    \n",
    "    # Mask data where no measurement exists: convert any data whose corresponding count is 0 to NaN\n",
    "    for column in lat_bins:\n",
    "        i+=1; update_progress(i / total_actions)\n",
    "        OLRdata[column] = OLRdata[column].mask(OLRdata_stats[column] < 1, np.nan)\n",
    "        OLRdata_stats[column]= OLRdata_stats[column].mask(OLRdata_stats[column] < 1, np.nan)\n",
    "    \n",
    "    statsdf[\"cell2\"][file]=OLRdata_stats.iloc[:, 2:].count().sum() # Store statistics after filtering 0's\n",
    "    \n",
    "    # Convert any data whose corresponding count is less than MIN_COUNT to NaN\n",
    "    if MIN_COUNT > 0:\n",
    "        for column in lat_bins:\n",
    "            i+=1; update_progress(i / total_actions)\n",
    "            OLRdata[column] = OLRdata[column].mask(OLRdata_stats[column] < MIN_COUNT, np.nan)\n",
    "            OLRdata_stats[column]= OLRdata_stats[column].mask(OLRdata_stats[column] < MIN_COUNT, np.nan)\n",
    "    statsdf[\"cell3\"][file]=OLRdata_stats.iloc[:, 2:].count().sum() # Store statistics after filtering <MIN_COUNT\n",
    "    \n",
    "    # Mask channels with fewer months than user minimum criteria\n",
    "    if MIN_MONTHS_TO_OBTAIN_SLOPE > 0:\n",
    "        for column in lat_bins:\n",
    "            col_series = OLRdata[column]\n",
    "            mask_series = pd.Series(False, index=OLRdata.index) # to keep track of items to be removed\n",
    "            for wn in wavenums:\n",
    "                i+=1; update_progress(i / total_actions)\n",
    "                timeseries = col_series[OLRdata['wavenumber'] == wn]\n",
    "                if (~np.isnan(timeseries)).sum() < MIN_MONTHS_TO_OBTAIN_SLOPE:   # If total non-NAN points is insufficient\n",
    "                    mask_series = (mask_series | (OLRdata['wavenumber'] == wn))  # add the matching points to our mask\n",
    "            total_masked += mask_series.sum()\n",
    "            OLRdata[column] = OLRdata[column].mask(mask_series, np.nan)\n",
    "            OLRdata_stats[column] = OLRdata_stats[column].mask(mask_series, np.nan)\n",
    "    statsdf[\"cell4\"][file]=OLRdata_stats.iloc[:, 2:].count().sum() # Store count after filter <MIN_MONTHS_TO_OBTAIN\n",
    "    statsdf2=statsdf2.append(OLRdata_stats)  # Append aggregate filtered states for later median determination\n",
    "    \n",
    "    all_radiances[file]={}\n",
    "    all_radiance_trends[file]={}\n",
    "    for lat_index, latitude in enumerate(lat_bins): # This is an enumerate call, so that we can index \n",
    "        df3 = OLRdata.set_index(['period', 'wavenumber'])[latitude].unstack() # Re-arrange so each wavenum in its own column\n",
    "        df3.index = pd.DatetimeIndex(df3.index) # Re-format the index's date strings into numerical dates\n",
    "        linmodels_dict = {} # Fitting linear models\n",
    "        dates = df3.index # Need to convert the dates into months in order to perform regression\n",
    "        mo = dates.year * 12 + dates.month\n",
    "        for col in df3.columns:\n",
    "            i+=1; update_progress(i / total_actions)\n",
    "            ser = df3[col] # Get the individual series\n",
    "            d = {}\n",
    "            varx = mo\n",
    "            vary = ser.values\n",
    "            mask = ~np.isnan(varx) & ~np.isnan(vary)\n",
    "            if sum(mask) > 0: # Perform linear regression\n",
    "                d['Slope'], d['Intercept'], d['R'], d['P-value'], d['Std. Err'] = stats.linregress(varx[mask], vary[mask])\n",
    "                d['avg'] = vary[mask].mean()\n",
    "                linmodels_dict[col] = d\n",
    "        linmodels_df = pd.DataFrame(linmodels_dict).transpose()  # Compile a DataFrame of the linear models, and save it\n",
    "        linmodels_df[\"R^2\"] = linmodels_df[\"R\"] ** 2     # Get R^2 for additional information\n",
    "        linmodels_df[\"Slope\"] = linmodels_df[\"Slope\"] * 12    # Convert slope monthly -> yearly\n",
    "        all_radiances[file][latitude] = pd.DataFrame(df3)\n",
    "        all_radiance_trends[file][latitude] = {\n",
    "            \"wavenumbers\": linmodels_df.index,\n",
    "            \"slope\": linmodels_df[\"Slope\"],\n",
    "            \"avg\": linmodels_df['avg'],\n",
    "        }\n",
    "update_progress(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percent of grid cells with no data:\", '{:.2f}'.format((1-(statsdf['cell2'].sum()/statsdf['cell1'].sum()))*100), '%')\n",
    "if MIN_COUNT > 0:\n",
    "    print(\"Grid cells <MIN_COUNT:\", '{:.2f}'.format((1-(statsdf['cell3'].sum()/statsdf['cell2'].sum()))*100), '%')\n",
    "    print(\"Grid cells <MIN_MONTHS:\", '{:.2f}'.format((1-(statsdf['cell4'].sum()/statsdf['cell3'].sum()))*100), '%')\n",
    "else:\n",
    "    print(\"Grid cells <MIN_MONTHS:\", '{:.2f}'.format((1-(statsdf['cell4'].sum()/statsdf['cell2'].sum()))*100), '%')\n",
    "print(\"Median radiance measurements in a grid cell:\", statsdf2.iloc[:, 2:].stack().median())\n",
    "print(\"Mean radiance measurements in a grid cell:\", statsdf2.iloc[:, 2:].stack().mean())\n",
    "print('Measurement count:', '{:.2f}'.format(statsdf2.iloc[:, 2:].sum(axis=0).sum()/1E9),'billion')\n",
    "Count_sub_1614=pd.DataFrame()\n",
    "Count_sub_1614=statsdf2.loc[statsdf2.loc[:,\"wavenumber\"]<1614]\n",
    "print('Measurement count <1614 cm-1:', '{:.2f}'.format(Count_sub_1614.iloc[:, 2:].sum(axis=0).sum()/1E9),'billion')\n",
    "Count_sub_765=pd.DataFrame()\n",
    "Count_sub_765=statsdf2.loc[statsdf2.loc[:,\"wavenumber\"]<765]\n",
    "print('Measurement count <765 cm-1:', '{:.2f}'.format(Count_sub_765.iloc[:, 2:].sum(axis=0).sum()/1E9),'billion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planck Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_planck(v, T):\n",
    "    '''\n",
    "    Receives: wavenumber, blackbody temperature\n",
    "    Returns: spectral radiance\n",
    "    equation from https://ncc.nesdis.noaa.gov/data/planck.html\n",
    "    '''\n",
    "    return ((0.00001191042*v**3)/(np.exp(1.4387752*v/T)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_planck(v, L): \n",
    "    '''\n",
    "    Receives: wavenumber, spectral radiance\n",
    "    Returns: Planck brightness temperature\n",
    "    equation from https://ncc.nesdis.noaa.gov/data/planck.html\n",
    "    '''\n",
    "    return (1.4387752*v/(np.log(0.00001191042*v**3/L+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area-weighted averages & error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planet regarded as a sphere, creating area-proportionate weighting factors\n",
    "lat_deg = np.arange(90, -1, -10).tolist()\n",
    "lat_rad = np.radians(lat_deg)\n",
    "lat_sin = np.sin(lat_rad)\n",
    "lat_dsin = lat_sin[0]-lat_sin\n",
    "cap_area = 2 * np.pi * lat_dsin\n",
    "cap_shift = np.insert(cap_area, 0,0)\n",
    "belt_area = cap_area - cap_shift[:-1]\n",
    "globe_area = np.concatenate([belt_area[1:], np.flip(belt_area[1:])])\n",
    "weights=globe_area/np.sum(globe_area)\n",
    "\n",
    "weights2=[]  # when processing multiple longitude bins, weight the result by # of longitude bins\n",
    "for latitude_weight in weights:\n",
    "    weights2.append(latitude_weight/len(file_list))\n",
    "\n",
    "assert round(sum(weights2)*len(file_list), 6) == 1, \"weights don't sum to 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgR = pd.DataFrame(0, index=wavenums, columns=lat_bins)\n",
    "series_averagedslopes = pd.Series(index=wavenums, data=0)\n",
    "lat_rad_err={}\n",
    "lat_rad_trends={}\n",
    "for latitude in lat_bins:\n",
    "    lat_rad_trends[latitude]= pd.Series(index=wavenums, data=0)\n",
    "    lat_rad_err[latitude] = pd.Series(index=wavenums, data=0)\n",
    "\n",
    "\n",
    "for lat_index, latitude in enumerate(lat_bins):\n",
    "    for file in file_list:\n",
    "        series_averagedslopes += all_radiance_trends[file][latitude]['slope'] * weights2[lat_index]\n",
    "        avgR[latitude] += all_radiance_trends[file][latitude]['avg'] / len(file_list)\n",
    "        lat_rad_err[latitude] += all_radiance_trends[file][latitude]['avg'] / len(file_list)\n",
    "        lat_rad_trends[latitude] += all_radiance_trends[file][latitude]['slope'] / len(file_list)\n",
    "    lat_rad_err[latitude] = inv_planck(lat_rad_err[latitude].index, lat_rad_err[latitude].values)\n",
    "    lat_rad_err[latitude] = fwd_planck(wavenums, (lat_rad_err[latitude].values + drift_K)) - fwd_planck(wavenums, lat_rad_err[latitude].values)\n",
    "\n",
    "\n",
    "avgR = np.sqrt(avgR**2 * weights**2)\n",
    "avgT = pd.Series(index=wavenums, data=inv_planck(avgR.index, avgR.sum(axis=1).values))\n",
    "series_averagederror = pd.Series(index=wavenums, data= fwd_planck(avgT.index, (avgT.values + drift_K)) - fwd_planck(avgT.index, avgT.values))\n",
    "mw_error = pd.Series(index=wavenums, data= fwd_planck(avgT.index, (avgT.values + drift_K2)) - fwd_planck(avgT.index, avgT.values))\n",
    "mw_error = mw_error.drop(mw_error.index[np.where(mw_error.index < 1800)[0]])\n",
    "series_averagederror.update(mw_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Radiance Trends (slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_df = pd.DataFrame(index = series_averagedslopes.index, data = {'slope':series_averagedslopes.values})\n",
    "slopes_df.index.name = 'WN'\n",
    "\n",
    "# 1. Remove NaNs by linear interpolation\n",
    "slopes_df1 = slopes_df.copy(deep=True)\n",
    "slopes_df1 = slopes_df1.interpolate(method = 'linear', limit = 10, limit_area = 'inside')\n",
    "\n",
    "# 2. Insert 0 value on ordinate where adjacent slopes are opposite sign\n",
    "slopes_df2 = slopes_df1.copy(deep=True)\n",
    "for i in range(340, 390, 1):\n",
    "    if ((slopes_df2['slope'].iloc[i] < 0 and slopes_df2['slope'].iloc[i+1] > 0) or (slopes_df2['slope'].iloc[i] > 0 and slopes_df2['slope'].iloc[i+1] < 0)):\n",
    "        y1, y2, y3 = slopes_df2['slope'].iloc[i], 0, slopes_df2['slope'].iloc[i+1]\n",
    "        x1, x3 = slopes_df2.index[i], slopes_df2.index[i+1]\n",
    "        slopes_df2.loc[(y2-y1)*(x3-x1)/(y3-y1) + x1] = 0\n",
    "slopes_df2 = slopes_df2.sort_index()\n",
    "\n",
    "# 3. Remove positive slope values, presuming they are transparent wavenumbers viewing surface heating (not atmosphere)\n",
    "slopes_df3 = slopes_df2.copy(deep=True)\n",
    "slopes_df3[slopes_df3 > 0] = 0\n",
    "\n",
    "slopes_interp = slopes_df[np.isnan(slopes_df['slope'])]  # dataframe of all NaNs\n",
    "slopes_interp.update(slopes_df1)  # Replace NaNs with actual slopes\n",
    "slopes_interp.index.name = 'WN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $v_2$ wing examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10.5,3))       \n",
    "ax[0].set_title(\"Raw v2 wing\")\n",
    "ax[0].fill_between(series_averagedslopes[v2wingstart:v2wingend].index,\n",
    "                   series_averagedslopes[v2wingstart:v2wingend].values, 0, color='grey', linewidth = 0)\n",
    "ax[1].set_title(\"Interpolated v2 wing\")\n",
    "ax[1].fill_between(slopes_df2[v2wingstart:v2wingend].index,\n",
    "                   slopes_df2[v2wingstart:v2wingend]['slope'].values, 0, color='grey', linewidth = 0)\n",
    "\n",
    "markerline, stemlines, baseline = ax[1].stem(slopes_interp[v2wingstart:v2wingend].index,\n",
    "                                             slopes_interp[v2wingstart:v2wingend]['slope'].values,\n",
    "                                             markerfmt=' ', linefmt=\"k-\", basefmt=\"grey\", use_line_collection = True)\n",
    "plt.setp(stemlines, 'linewidth', 0.25)\n",
    "plt.setp(baseline, 'linewidth', 0)\n",
    "\n",
    "ax[2].set_title(\"Negative-only v2 wing\")\n",
    "ax[2].fill_between(slopes_df3[v2wingstart:v2wingend].index, \n",
    "                   slopes_df3[v2wingstart:v2wingend]['slope'].values, 0, color='grey', linewidth = 0)\n",
    "ax[0].set_ylim(-.1, 0.05)\n",
    "ax[1].set_ylim(-.1, 0.05)\n",
    "ax[2].set_ylim(-.1, 0.05)\n",
    "fig.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $v_3$ band examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10.5,3))\n",
    "ax[0].set_title(\"Raw v3 band\")\n",
    "ax[0].fill_between(series_averagedslopes[v3start:v3end].index,\n",
    "                   series_averagedslopes[v3start:v3end].values, 0, color='grey', linewidth = 0)\n",
    "ax[1].set_title(\"Interpolated v3 band\")\n",
    "ax[1].fill_between(slopes_df2[v3start:v3end].index, \n",
    "                   slopes_df2[v3start:v3end]['slope'].values, 0, color='grey', linewidth = 0)\n",
    "\n",
    "markerline, stemlines, baseline = ax[1].stem(slopes_interp[v3start:v3end].index,\n",
    "                                             slopes_interp[v3start:v3end]['slope'].values,\n",
    "                                             markerfmt=' ', linefmt=\"k-\", basefmt=\"grey\", use_line_collection = True)\n",
    "plt.setp(stemlines, 'linewidth', 0.25)\n",
    "plt.setp(baseline, 'linewidth', 0)\n",
    "\n",
    "ax[2].set_title(\"Negative-only v3 band\")\n",
    "ax[2].fill_between(slopes_df3[v3start:v3end].index,\n",
    "                   slopes_df3[v3start:v3end]['slope'].values, 0, color='grey', linewidth = 0)\n",
    "ax[0].set_ylim(-.0007, 0.00015)\n",
    "ax[1].set_ylim(-.0007, 0.00015)\n",
    "ax[2].set_ylim(-.0007, 0.00015)\n",
    "fig.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Integral (1)-(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put series data into a dataframe so that we may evaluate integral with scipi.integrate.trapz:\n",
    "rad_err_df = pd.DataFrame(index = series_averagederror.index, data = {'slope':series_averagederror.values})\n",
    "\n",
    "# Evaluate window integral by Trapezoidal rule\n",
    "v2_win = scipy.integrate.trapz(slopes_df3[win_start:win_end]['slope'].values, slopes_df3[win_start:win_end].index) * math.pi\n",
    "v2_win_err = scipy.integrate.trapz(rad_err_df[win_start:win_end]['slope'].values, rad_err_df[win_start:win_end].index) * math.pi\n",
    "\n",
    "# Evaluate v2 PQR integral by Trapezoidal rule\n",
    "v2_PQR = scipy.integrate.trapz(slopes_df3[v2start:v2end]['slope'].values, slopes_df3[v2start:v2end].index) * math.pi\n",
    "v2_PQR_err = scipy.integrate.trapz(rad_err_df[v2start:v2end]['slope'].values,\n",
    "                                   rad_err_df[v2start:v2end].index) * math.pi\n",
    "# Evaluate v2 PQR integral by Simpson's rule\n",
    "v2_PQRS = scipy.integrate.simps(slopes_df3[v2start:v2end]['slope'].values, slopes_df3[v2start:v2end].index) * math.pi\n",
    "v2_PQR_errS = scipy.integrate.simps(rad_err_df[v2start:v2end]['slope'].values, rad_err_df[v2start:v2end].index) * math.pi\n",
    "\n",
    "# Evaluate v2 wing integral by Trapezoidal rule\n",
    "v2_wing = scipy.integrate.trapz(slopes_df3[v2wingstart:v2wingend]['slope'].values,\n",
    "                                slopes_df3[v2wingstart:v2wingend].index) * math.pi\n",
    "v2_wing_err = scipy.integrate.trapz(rad_err_df[v2wingstart:v2wingend]['slope'].values,\n",
    "                                    rad_err_df[v2wingstart:v2wingend].index) * math.pi\n",
    "# Evaluate v2 wing integral by Simpson's rule\n",
    "v2_wingS = scipy.integrate.simps(slopes_df3[v2wingstart:v2wingend]['slope'].values,\n",
    "                                 slopes_df3[v2wingstart:v2wingend].index) * math.pi\n",
    "v2_wing_errS = scipy.integrate.simps(rad_err_df[v2wingstart:v2wingend]['slope'].values,\n",
    "                                     rad_err_df[v2wingstart:v2wingend].index) * math.pi\n",
    "\n",
    "# Evaluate v3 integral by Trapezoidal rule\n",
    "v3 = scipy.integrate.trapz(slopes_df3[v3start:v3end]['slope'].values, slopes_df3[v3start:v3end].index) * math.pi\n",
    "v3_err = scipy.integrate.trapz(rad_err_df[v3start:v3end]['slope'].values, rad_err_df[v3start:v3end].index) * math.pi\n",
    "# Evaluate v3 integral by Simpson's rule\n",
    "v3S = scipy.integrate.simps(slopes_df3[v3start:v3end]['slope'].values, slopes_df3[v3start:v3end].index) * math.pi\n",
    "v3_errS = scipy.integrate.simps(rad_err_df[v3start:v3end]['slope'].values, rad_err_df[v3start:v3end].index) * math.pi\n",
    "\n",
    "# Quantify R-branch measurement gap\n",
    "Interpolate_integral=scipy.integrate.trapz(slopes_df3[v2end-5.1:v2end]['slope'].values, \n",
    "                                           slopes_df3[v2end-5.1:v2end].index) * math.pi\n",
    "Interpolate_int_err = scipy.integrate.trapz(rad_err_df[v2end-5.1:v2end]['slope'].values,\n",
    "                                            rad_err_df[v2end-5.1:v2end].index) * math.pi\n",
    "\n",
    "print('v2 win integral (Trapz) =','{:.2f}'.format(v2_win),'±','{:.2f}'.format(v2_win_err),'mW/m²/yr')\n",
    "print('v2 PQR integral (Trapz) =','{:.2f}'.format(v2_PQR+Interpolate_integral),'±','{:.2f}'.format(v2_PQR_err+Interpolate_int_err),'mW/m²/yr')\n",
    "print('v2 PQR integral (Simps) =','{:.2f}'.format(v2_PQRS+Interpolate_integral),'±','{:.2f}'.format(v2_PQR_errS+Interpolate_int_err),'mW/m²/yr')\n",
    "print('v2 wing integral (Trapz) =','{:.2f}'.format(v2_wing),'±','{:.2f}'.format(v2_wing_err),'mW/m²/yr')\n",
    "print('v2 wing integral (Simps) =','{:.2f}'.format(v2_wingS),'±','{:.2f}'.format(v2_wing_errS),'mW/m²/yr')\n",
    "print('v3 integral     (Trapz) =','{:.2f}'.format(v3),'±','{:.2f}'.format(v3_err),'mW/m²/yr')\n",
    "print('v3 integral     (Simps) =','{:.2f}'.format(v3S),'±','{:.2f}'.format(v3_errS),'mW/m²/yr')\n",
    "\n",
    "AIRS_total =  (v2_PQR + 2 * v2_wing + Interpolate_integral + v3 + v2_win) * year_count\n",
    "AIRS_err =  (v2_PQR_err + 2 * v2_wing_err  + v3_err + v2_win + Interpolate_int_err) * year_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations (1)-(4) and $\\delta$M<sub>LW</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v2 PQR integral  =','{:.0f}'.format((v2_PQR+Interpolate_integral)*year_count),'±','{:.0f}'.format((v2_PQR_err+Interpolate_int_err)*year_count),'mW/m²/yr')\n",
    "print('v2 wing integral =','{:.0f}'.format(v2_wing*year_count),'±','{:.0f}'.format(v2_wing_err*year_count),'mW/m²/yr')\n",
    "print('v2 win integral  =','{:.1f}'.format(v2_win*year_count),'±','{:.1f}'.format(v2_win_err*year_count),'mW/m²/yr')\n",
    "print('v3 integral      =','{:.0f}'.format(v3*year_count),'±','{:.0f}'.format(v3_err*year_count),'mW/m²/yr')\n",
    "print(\"\")\n",
    "print('Total:', '{:.0f}'.format(AIRS_total), '±','{:.0f}'.format(AIRS_err),'mW/m²')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CO<sub>2</sub> measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During AIRS observation period, atmospheric CO2 given by NOAA ESRL Global Division\n",
    "# https://www.esrl.noaa.gov/gmd/ccgg/trends/gl_data.html\n",
    "# start = 373.13 (Trend value September, 2002)\n",
    "# end = 410.21 (Trend value Aug, 2019)\n",
    "\n",
    "CO2_start = 373.13  # ppm CO2 where AIRS data begins\n",
    "CO2_end = 410.21    # ppm CO2 where AIRS data ends\n",
    "\n",
    "print(\"CO2 start:\", CO2_start)\n",
    "print(\"CO2 end:\", CO2_end)\n",
    "print(\"delta:\",'{:.1f}'.format(CO2_end-CO2_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_MYHRE98(C, C0):\n",
    "    return 5.35 * np.log(C / C0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CMIP6 4xCO<sub>2</sub> and scale down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMIP6_ERF_LW_cs=pd.read_csv(\"./data/ERF_LW_cs_4xCO2.csv\") # CMIP6 Data from Chris Smith\n",
    "print(CMIP6_ERF_LW_cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMIP6 = CMIP6_ERF_LW_cs\n",
    "CMIP6['M98x1.1'] = round(F_MYHRE98(CO2_end, CO2_start) / F_MYHRE98(4*278, 278) * CMIP6['ERF_LW_cs'], 3)\n",
    "CMIP6['M98%AIRS'] = round((CMIP6['M98x1.1']/(-1* AIRS_total/1000)-1)*100, 0)\n",
    "CMIP6 = CMIP6.drop(['Run'], axis=1)\n",
    "CMIP6 = CMIP6.drop(CMIP6.index[[9, 12, 13, 14, 16]])\n",
    "CMIP6 = CMIP6.reset_index(drop=True)\n",
    "print(CMIP6)\n",
    "CMIP6_avg = CMIP6['M98x1.1'].mean()\n",
    "print('CMIP6 avg:', round(CMIP6_avg, 3),\"   +\", round((CMIP6_avg/(-1*AIRS_total/1000)-1)*100, 0),'% vs AIRS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What percent of CMIP6 CO<sub>2</sub> forcing is actually observed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AIRS:', '{:.0f}'.format(-1*AIRS_total), '±','{:.0f}'.format(AIRS_err),'mW/m²')\n",
    "print('CMIP6:', '{:.0f}'.format(CMIP6_avg*1000) ,'mW/m²')\n",
    "print('AIRS/CMIP6 =','{:.0f}'.format(AIRS_total / (CMIP6_avg*-1000)*100), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- END OF DATA PROCESSING ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To recreate Figure 1:\n",
    "project a dotted box from +/- 60 lat, -130 to -190 long for Harries pseduo-global test area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To recreate Figure 2:\n",
    "plot the 'radiances' field for 'hdffile' at Track=0, xTrack=0 for AIRS.2016.01.05.004.L2.CC_IR.v6.0.31.0.G16005164855.hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To recreate Figure 3:\n",
    "Highlight areas of AIRS.2016.01.01.015.L1B.VIS_Rad.v5.0.0.0.G16001115936.hdf.jpg where Tot_CldCC4 = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To recreate Figure 4:\n",
    "plot channels 982.838, 668.288, 740.973, 650.814 from OLRdata at -10to0 lat, 20to40 lon with stats.linregress on each series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To recreate Figure 5:\n",
    "Plot lat_rad_trends.index, lat_rad_trends for each latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To recreate Figure 6:\n",
    "Plot series_averagedslopes and shaded +/- series_averagederror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To recreate Figure 7:\n",
    "plot HITRAN lines between 450-900 cm-1 from spectralcalc.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To recreate Figure 8:\n",
    "Plot 650-750 cm-1 from row 13 of granule 004 from Jan 5, 2016 (all radiances are relative to the average of (X-track(15)+X-track(16))/2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
